{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBsxGkWO62hNztVL87Ab0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayberkcorp/cap4630-artificial_intelligence/blob/master/HW_5/Homework5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPcX8xwjSzJR",
        "colab_type": "text"
      },
      "source": [
        "# Problem 1\n",
        "\n",
        "### 1. General concepts:\n",
        "In this course, I learned that artificial intelligence consists mostly of various algorithms to manipulate the computer into doing tasks that only intelligent entities would be able to complete. I learned that machine learning is a subset of artificial intelligence and that deep learning, which was the main focus of this class, is a subset of machine learning.\n",
        "\n",
        "Machine learning allows the computer to learn things without having been explicitly programmed to do so. It creates a dynamic environment in which computers can adjust themselves and the choices they make based on the data that they are fed.\n",
        "\n",
        "Deep learning takes the game to the next level and we see that the computer relies on networks of algorithms called artificial neural networks, in which each algorithm provides a different interpretation of the data to extract higher level features from it. This idea stems from the model of the biological brain during information processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sFNRo_dS5vX",
        "colab_type": "text"
      },
      "source": [
        "### 2. Basic concepts:\n",
        "\n",
        "A key tool in machine learning is the regression model which predicts continuous values. The linear regression model uses a linear equation of weights and features to make inferences about linear data.\n",
        "\n",
        "Logistic regression models are used in binary classification problems. For this type of model we need to use the sigmoid function: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, which allows us to attach probability values to each piece of data we have. We then split the probabilities and our data into two classes like so:\n",
        "\n",
        "$\\begin{gather}\n",
        "\\text{class 0 if } \\sigma(z) < \\frac{1}{2} \\\\\n",
        "\\text{class 1 if } \\sigma(z) \\geq \\frac{1}{2}\n",
        "\\end{gather}$\n",
        "\n",
        "The most important step when making inferences is mitigating your loss, which happens to be a core concept in machine learning. For linear regressions, we used mean squared error for our loss function and binary cross entropy loss for logistic regressions.\n",
        "\n",
        "Gradient descent is the main algorithm used for lowering the total loss when making predictions on data. We use loss functions that have a defined minimum, even when dealing with higher-dimensional data. After computing the loss for one particular set of weights, we adjust those weights to bring the loss down by looking in the direction of the negative gradient of the loss function. How much we adjust the values on each iteration depends on the user-set learning rate.\n",
        "\n",
        "The reason we have to proceed in the direction of the negative gradient of the loss function is because the gradient points in the direction of steepest ascent. We need to decrease loss for our models so we must go in the exact opposite (negative) direction to achieve our minimum loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlceR_vQS_rN",
        "colab_type": "text"
      },
      "source": [
        "### 3. Building a model:\n",
        "I learned how to build models using a convolutional neural network (CNN) which consists of a convolutional base and a classifier. The structure of a CNN can be described as a stack of modules, each of which performs various operations on an input matrix.\n",
        "\n",
        "The first module of the network generally performs a convolution on the matrix, during which a filter matrix is applied all around the input matrix to compute new features and produce an output feature map. Afterwards, the CNN applies a Rectified Linear Unit (ReLU) transformation to the convolved feature to introduce nonlinearity into the model. Finally, an algorithm called max pooling is used to reduce the dimensions of the output feature map to save on processing time while preserving information.\n",
        "\n",
        "To perform classification based on the features extracted by the convolutions, we have to include one or more fully connected layers at the end of the CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBpTgsONTCBU",
        "colab_type": "text"
      },
      "source": [
        "### 4. Compiling a model:\n",
        "\n",
        "Keras allows us to use different optimizers to specify the updates to the parameters of the model on each iteration. Two notable optimizers we used during this class were the RMSprop and stochastic gradient descent optimizers.\n",
        "\n",
        "One of the parameters that is necessary to specify when compiling a model is the learning rate which was already mentioned above. As the name suggests, this parameter specifies how quickly the model should learn. This value is multiplied by the magnitude of the current gradient of the loss function for our model to incrementally bring us closer to the minimum of this function.\n",
        "\n",
        "One important parameter of the stochastic gradient descent optimizer is the momentum field. This value accelerates gradient descent in the relevant direction and dampens oscillations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU2xB_APTKqg",
        "colab_type": "text"
      },
      "source": [
        "### 5. Training a model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9QAdWlfTOTQ",
        "colab_type": "text"
      },
      "source": [
        "### 6. Finetuning a pretrained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMZmcxhciNDq",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/1_a_slides.pdf\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/logistic_regression.pdf\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_c_slides.pdf\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_e_slides.pdf\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/CNN_slides.pdf\n",
        "\n",
        "https://keras.io/optimizers/"
      ]
    }
  ]
}