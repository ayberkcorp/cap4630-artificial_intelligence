{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnYQiMzksy3HyMnSNT3hQ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayberkcorp/cap4630-artificial_intelligence/blob/master/HW_5/Homework5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPcX8xwjSzJR",
        "colab_type": "text"
      },
      "source": [
        "# Problem 1\n",
        "\n",
        "### 1. General concepts:\n",
        "In this course, I learned that artificial intelligence consists mostly of various algorithms to manipulate the computer into doing tasks that only intelligent entities would be able to complete. I learned that machine learning is a subset of artificial intelligence and that deep learning, which was the main focus of this class, is a subset of machine learning.\n",
        "\n",
        "Machine learning allows the computer to learn things without having been explicitly programmed to do so. It creates a dynamic environment in which computers can adjust themselves and the choices they make based on the data that they are fed.\n",
        "\n",
        "Deep learning takes the game to the next level and we see that the computer relies on networks of algorithms called artificial neural networks, in which each algorithm provides a different interpretation of the data to extract higher level features from it. This idea stems from the model of the biological brain during information processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sFNRo_dS5vX",
        "colab_type": "text"
      },
      "source": [
        "### 2. Basic concepts:\n",
        "\n",
        "A key tool in machine learning is the regression model which predicts continuous values. The linear regression model uses a linear equation of weights and features to make inferences about linear data.\n",
        "\n",
        "Logistic regression models are used in binary classification problems. For this type of model we need to use the sigmoid function: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, which allows us to attach probability values to each piece of data we have. We then split the probabilities and our data into two classes like so:\n",
        "\n",
        "$\\begin{gather}\n",
        "\\text{class 0 if } \\sigma(z) < \\frac{1}{2} \\\\\n",
        "\\text{class 1 if } \\sigma(z) \\geq \\frac{1}{2}\n",
        "\\end{gather}$\n",
        "\n",
        "The most important step when making inferences is mitigating your loss, which happens to be a core concept in machine learning. For linear regressions, we used mean squared error for our loss function and binary cross entropy loss for logistic regressions.\n",
        "\n",
        "Gradient descent is the main algorithm used for lowering the total loss when making predictions on data. We use loss functions that have a defined minimum, even when dealing with higher-dimensional data. After computing the loss for one particular set of weights, we adjust those weights to bring the loss down by looking in the direction of the negative gradient of the loss function. How much we adjust the values on each iteration depends on the user-set learning rate.\n",
        "\n",
        "The reason we have to proceed in the direction of the negative gradient of the loss function is because the gradient points in the direction of steepest ascent. We need to decrease loss for our models so we must go in the exact opposite (negative) direction to achieve our minimum loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlceR_vQS_rN",
        "colab_type": "text"
      },
      "source": [
        "### 3. Building a model:\n",
        "I learned how to build models using a convolutional neural network (CNN) which consists of a convolutional base and a classifier. The structure of a CNN can be described as a stack of modules, each of which performs various operations on an input matrix.\n",
        "\n",
        "The first module of the network generally performs a convolution on the matrix, during which a filter matrix is applied all around the input matrix to compute new features and produce an output feature map. Afterwards, the CNN applies a Rectified Linear Unit (ReLU) transformation to the convolved feature to introduce nonlinearity into the model. Finally, an algorithm called max pooling is used to reduce the dimensions of the output feature map to save on processing time while preserving information.\n",
        "\n",
        "To perform classification based on the features extracted by the convolutions, we have to include one or more fully connected layers at the end of the CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBpTgsONTCBU",
        "colab_type": "text"
      },
      "source": [
        "### 4. Compiling a model:\n",
        "\n",
        "Keras allows us to use different optimizers to specify the updates to the model on each iteration. Two notable optimizers we used during this class were the RMSprop and stochastic gradient descent optimizers.\n",
        "\n",
        "One of the parameters that is necessary to specify when compiling a model is the learning rate which was already mentioned above. As the name suggests, this parameter specifies how quickly the model should learn. This value is multiplied by the magnitude of the current iteration's gradient of the loss function for our model to incrementally bring us closer to the minimum of this function.\n",
        "\n",
        "Another important parameter that is passed to the stochastic gradient descent optimizer is the momentum field. This value accelerates gradient descent in the relevant direction and dampens oscillations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU2xB_APTKqg",
        "colab_type": "text"
      },
      "source": [
        "### 5. Training a model:\n",
        "\n",
        "When training a model, overfitting is a major problem to be aware of. Overfitting occurs when a model tries to fit the training data so closely that it does not generalize well to new data. A model in which overfitting occurs during the training phase will have a low loss on the training data, but high loss when making predictions on test data. This is why we are not looking to completely eliminate loss when training the model.\n",
        "\n",
        "Underfitting, the opposite of overfitting, occurs when we have too much loss on training data. This happens because the model leaves much of the training data unaccounted for.\n",
        "\n",
        "I associate overfitting with the phrase \"too particular\" and underfitting with the phrase \"too general\" as a way to remember the distinction between them.\n",
        "\n",
        "Another concept I learned about when training models was the use of hyperparameters which are determined before the training phase and are not updated during training.\n",
        "\n",
        "Two examples of hyperparameters that I learned about are the learning rate, which is specified in the compilation phase as noted above, and the epoch count. In addition to what was mentioned above, it is important to set a good learning rate since one that is too high can cause the model to skip over the minimum of the loss function and one that is too low can create a time delay when training the model. The epoch count is simply the number of iterations the model should go through during training. Each iteration generally goes through the training data set one time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9QAdWlfTOTQ",
        "colab_type": "text"
      },
      "source": [
        "### 6. Finetuning a pretrained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMZmcxhciNDq",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/1_a_slides.pdf\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/logistic_regression.pdf\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_c_slides.pdf\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_e_slides.pdf\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/CNN_slides.pdf\n",
        "\n",
        "https://keras.io/optimizers/\n",
        "\n",
        "https://github.com/schneider128k/machine_learning_course/blob/master/slides/5_slides.pdf"
      ]
    }
  ]
}